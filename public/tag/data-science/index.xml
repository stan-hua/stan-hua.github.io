<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science | Stanley Z. Hua</title>
    <link>https://stanhua.rbind.io/tag/data-science/</link>
      <atom:link href="https://stanhua.rbind.io/tag/data-science/index.xml" rel="self" type="application/rss+xml" />
    <description>Data Science</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 04 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://stanhua.rbind.io/img/me</url>
      <title>Data Science</title>
      <link>https://stanhua.rbind.io/tag/data-science/</link>
    </image>
    
    <item>
      <title>PCA for Dimensionality Reduction</title>
      <link>https://stanhua.rbind.io/post/pca-part-i/pca_part_1/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://stanhua.rbind.io/post/pca-part-i/pca_part_1/</guid>
      <description>&lt;style&gt;
body {font-size: 80%}
.note{font-size: 10pt;
      line-height: 20pt;
      padding-bottom: 10px}
p{text-indent: 2em;}

details{font-size: 10pt;}
summary{font-size: 100% !important;}
&lt;/style&gt;
&lt;hr&gt;
&lt;h3&gt;OVERVIEW&lt;/h3&gt;
&lt;p&gt;Principle Component Analysis (PCA) is known for two things: 1) Dimensionality Reduction, and 2) Structure Analysis. Note that it can also be used for &lt;strong&gt;factor extraction&lt;/strong&gt;, the first step in Factor Analysis.&lt;/p&gt;
&lt;div class=&#34;note&#34;&gt;
&lt;b&gt;NOTE&lt;/b&gt;: There are plenty of resources online if you wish to learn more about Factor Analysis, but they will not be covered here!
&lt;/div&gt;
&lt;h4&gt;TERMINOLOGY&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;High dimensionality&lt;/strong&gt; means that the data has a lot of variables. Think 1000+ column variables!&lt;/p&gt;
&lt;p&gt;The goal of &lt;strong&gt;Dimensionality Reduction&lt;/strong&gt; is to reduce the number of dimensions, while retaining as much useful information from the original data as possible.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Example and Enrichment&lt;/summary&gt;
&lt;p&gt;Say you wish to visualize your data, in order to get an understanding of the relationships between each feature. However, you have too many features. It becomes impossible to plot them on an x-y graph. How do you visualize this without destroying your computer let alone the laws of physics? You can reduce the number of dimensions to 2, then plot it on a coordinate plane! &lt;/p&gt;
&lt;div class=&#34;note&#34;&gt;
&lt;b&gt;NOTE&lt;/b&gt;: Dimensionality Reduction is different from &lt;i&gt;Feature Selection&lt;/i&gt; (e.g. L1 Regularization, L2 Regularization, etc.). The goal of feature selection is to select the most important features. Meanwhile, dimensionality reduction is used to project high-dimensional data onto a set of low-dimensional features.
&lt;/div&gt;
&lt;/details&gt;
&lt;br&gt;
&lt;p&gt;Given p features, &lt;strong&gt;Principal Components&lt;/strong&gt; are the [1, min(n, p)] new variables extracted from n features using Principal Component Analysis. You can think of them as new axes to view the original data. &lt;em&gt;However, you can no longer interpret the principal components the same way you did with previous features&lt;/em&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Enrichment&lt;/summary&gt;
If you have 10 features and you transform them via PCA to get 10 principal components, they are not the same. If you previously had age and weight, now you simply have axes for data points with no interpretable meaning unlike the original variables. With PCA, you may end up with combinations of these variables. Tell me, is there meaning in a new variable that is 30% age and 70% weight?
&lt;/details&gt;
&lt;br&gt;&lt;br&gt;
&lt;h3&gt;GENERAL IDEA&lt;/h3&gt;
&lt;p&gt;To build intuition about what happens in Principal Component Analysis, let&amp;rsquo;s use the graph below as an example.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://stanhua.rbind.io/post/PCA-Part-I/2020-10-10-pca-for-dummies.en_files/figure-html/plot-1.png&#34; width=&#34;336&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, we have 2 features. You can imagine each point representing an observation (e.g. one of your participants).&lt;/p&gt;
&lt;p&gt;When you implement PCA, imagine a line placed at the mean (of all the data points). Imagine rotating it, and every time that you rotate it, it changes how far each observation (i.e. its projection onto the line) point is from the center of the line. This is also called the &lt;strong&gt;squared distances&lt;/strong&gt;. And this is what we&amp;rsquo;re trying to &lt;em&gt;maximize&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;By doing so, it is as if we are trying to find a line where the observations are farthest from center. Thus, we capture the most amount of variation along this line, and this line is known as a principal component!&lt;/p&gt;
&lt;p&gt;Once again, notice how the features no longer have anything to do with the new axes (principal component). Also notice that if you had 3 or more features, graphing it like we did is no longer possible.&lt;/p&gt;
&lt;h3 id=&#34;example-code&#34;&gt;Example Code&lt;/h3&gt;
&lt;p&gt;We provide a brief example of how to perform PCA on your data in Python. 
Be aware of a few things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;When you have less rows (observations) than columns (variables), you can at most have the number of dimensions as the number of rows.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For intuition, we can only fit at most m different lines through those m data points (observations).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If your variables are scaled differently, it is advisable to standardize your data first.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each column may be standardized to have a mean of 0 and standard deviation of 1.&lt;/li&gt;
&lt;li&gt;e.g. column A has values greater than 10,000, while column B has values around 50.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;import pandas as pd
from sklearn.decomposition import PCA


def reduce_dim(X, dims=2) -&amp;gt; pd.DataFrame:
    &amp;quot;&amp;quot;&amp;quot;
    Given a dataframe with n variables and m rows, perform PCA
    and project X to reduce the dimensionality.
    
    Parameters
    ----------
    X : pd.Dataframe or array-like
      A table of (n columns x m rows).
    dims: int
      The desired number of variables (i.e. dimensionality)
      after transforming &amp;lt;X&amp;gt;.
    
    Returns
    -------
    pd.DataFrame
      The input X projected onto (&amp;lt;dims&amp;gt; columns x m rows).
    &amp;quot;&amp;quot;&amp;quot;
    pca = PCA(n_components=dims)
    X_transformed = pd.DataFrame(pca.fit_transform(X))
    return X_transformed
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Thanks for reading!&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Additional Resources&lt;/h3&gt;
&lt;p&gt;StatQuest has great videos explaining what I mentioned with the graph with more detail! (&lt;a href=&#34;https://www.youtube.com/watch?v=FgakZw6K1QQ&#34;&gt;https://www.youtube.com/watch?v=FgakZw6K1QQ&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intro to Dimensionality Reduction</title>
      <link>https://stanhua.rbind.io/post/intro-dim-red/intro_to_dimred/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://stanhua.rbind.io/post/intro-dim-red/intro_to_dimred/</guid>
      <description>


&lt;style&gt;
h1{font-size: 165%;}
body {font-size: 80%}
.note{font-size: 10pt;
      line-height: 20pt;
      padding-bottom: 10px}
p{text-indent: 2em;}
details{font-size: 10pt;}
summary{font-size: 100% !important;}
&lt;/style&gt;
&lt;p&gt;Imagine a newly-founded insurance company collects data on their customers (e.g. their sex, age, occupation, location). With this information, the company would like to group their customers, in order to ascertain who are more likely to have risks to their health. To make a profit, the executives believe that customers in high-risk groups should be charged more expensive premiums. They decide to outsource this problem to you. Now, you are given a table with 100,000 customers and 1000 different column variables. What do you do?&lt;/p&gt;
&lt;p&gt;If you’re stubborn, you can try to compare all 1000 variables for all 100,000 customers, but this is an impossible task, possibly even for a machine. There are simply way too many &lt;em&gt;dimensions&lt;/em&gt; or variables to look at. What if you could inspect 2 or 3 variables instead? Though still a Herculean task for any human, it’s far more reasonable to compare two numbers than 1000.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;This brings us to the &lt;strong&gt;Curse of Dimensionality&lt;/strong&gt;, which tells us that as the number of dimensions in our data increase, the distance between our data points are going to become equal. This is important! Most methods of grouping people (or clustering data points) is based on &lt;em&gt;Euclidean&lt;/em&gt; distance between points. Intuitively, this is to say that the customers cannot be differentiated from one another, so how do you cluster them into groups?&lt;/p&gt;
&lt;p&gt;Generally, there are two ways you could &lt;strong&gt;&lt;em&gt;reduce&lt;/em&gt;&lt;/strong&gt; the &lt;strong&gt;&lt;em&gt;dimensionality&lt;/em&gt;&lt;/strong&gt; of your data. You can either select the top n number of most impactful variables related to health risk, such as age and occupation. Or you could represent those variables in another way. I am referring to 1) Feature Selection, and 2) Dimensionality Reduction. Make no mistake. They are very different methods!&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Enrichment&lt;/summary&gt;
&lt;div class=&#34;note&#34;&gt;
&lt;p&gt;&lt;b&gt;NOTE&lt;/b&gt;: The &lt;em&gt;new&lt;/em&gt; variables are combinations of the old variables. They will have no interpretable meaning. In feature selection, you’re simply keeping some variables and dropping the rest. Those variables retain their meaning!&lt;/p&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The one I will be discussing soon is &lt;strong&gt;Dimensionality Reduction&lt;/strong&gt;. The goal of which is to &lt;strong&gt;&lt;em&gt;represent&lt;/em&gt;&lt;/strong&gt; as much &lt;strong&gt;&lt;em&gt;useful&lt;/em&gt;&lt;/strong&gt; information in your data with the fewest number of dimensions.&lt;/p&gt;
&lt;p&gt;Imagine all the useful information about your participants could be represented with one or more &lt;em&gt;new&lt;/em&gt; variables that can explain almost as much as those 1000 variables. That is the essence of dimensionality reduction! Now, you can proceed with grouping the company’s customers.&lt;/p&gt;
&lt;p&gt;In the next post, we will focus on building intuition for Principal Component Analysis, a frequently used dimensionality reduction method. See you there!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
